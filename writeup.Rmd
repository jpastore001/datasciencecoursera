
**Weight Lifting Data Project for Machine Learning**

The goal of this project is to predict the manner in which a group of weight lifters did an exercise by observing the "classe" variable in the training set and using it as the dependent training variable.

As per the assignment this writeup walks through the methodolgy for training a model to predict the classe variable for new unobserved trials.

The first step is to read raw data into R:

```{r eval = FALSE}
# Get raw files from Coursera

ad_tr <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
ad_tst <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- "./data/pml-training.csv"
download.file(ad_tr, raw_train)
raw_test <- "./data/pml-testing.csv"
download.file(ad_tst, raw_test)

# Read train csv to dataframe
df.train <- read.csv("./data/pml-training.csv", na.strings = "NA", stringsAsFactors = F)
df.test <- read.csv("./data/pml-testing.csv", na.strings = "NA", stringsAsFactors = F)

```

Next I drop derived fields and pull out the independent variable to a separate variable.

```{r eval = FALSE}

#drop derived vars
x <- grep("^max|^min|^ampl|^var|^avg|^stdd|^ske|^kurt", names(df.train))
df.train <- df.train[-x]
df.test <- df.test[-x]

#conv y to factor from chr
df.train$classe <- factor(df.train$classe)

#split features and y
y.tr=df.train$classe
keep <- names(df.train)[8:59]
df.train=df.train[keep]
df.test=df.test[keep]
```

Next I partition the training data into two distinct sets using a 90/10 break for training and test respectively.  Since I later use k-fold cross validation it is not essential to have a test set at all really.  10% is useful to get additional out-of-sample stats and is plenty big since I am using k-fold cross-validation during training.

```{r eval = FALSE}
# Partition training data
inTrain <- createDataPartition(y.tr, p = .9, list = F)
training <- df.train[inTrain, ]
testing <- df.train[-inTrain, ]
y.training <- y.tr[inTrain]
y.testing <- y.tr[-inTrain]
```

I now preprocess the data applying principle components, Box-Cox transform, centering, scaling, and imputation of missing values.

```{r eval = FALSE}
# Preprocess data
p=preProcess(x = training, method = c("BoxCox", "center", "scale", "knnImpute", "pca"), thresh = 0.95)
training.p <- predict(p, training)
testing.p <- predict(p, testing)
print(p)
```

Result:
```{r eval = FALSE}
Call:
preProcess.default(x = training, method = c("BoxCox", "center",
 "scale", "knnImpute", "pca"), thresh = 0.95)

Created from 17662 samples and 52 variables
Pre-processing: Box-Cox transformation, centered, scaled, 5 nearest
 neighbor imputation, principal component signal extraction 

Lambda estimates for Box-Cox transformation:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
  0.900   1.175   1.450   1.450   1.725   2.000      50 

PCA needed 25 components to capture 95 percent of the variance

```

Specify 5-Fold cross-validation and train the model using random-forest.  After trying a bunch of different models I found random-forest to work best here.

```{r eval = FALSE}
# 5-fold cross-validation
tc <- trainControl(method = "cv", number = 5)

# tain model using random-forest model
model=train(training.p, y.training, method="rf", trControl = tc, verbose=F)
model
```

Result:
```{r eval = FALSE}
Random Forest 

17662 samples
   25 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 

Summary of sample sizes: 14130, 14129, 14129, 14130, 14130 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9767860  0.9706329  0.002582400  0.003265322
  13    0.9724829  0.9651907  0.003845879  0.004862116
  25    0.9697652  0.9617523  0.003581732  0.004533081

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2.
```

Next I use the trained model to predict classe variable for test datasets.

```{r eval = FALSE}
training.pred <- predict(model, training.p)
testing.pred <- predict(model, testing.p)

# apply same preprocessing from training set
testset.p <- predict(p, df.test)
testset.predict<-predict(model, testset.p)

cm.test=confusionMatrix(testing.pred,y.testing)
cm.test
```

Result:
```{r eval = FALSE}
Confusion Matrix and Statistics

          Reference
Prediction   A   B   C   D   E
         A 556   5   0   0   0
         B   0 371   5   0   1
         C   0   0 335   9   5
         D   2   0   2 312   2
         E   0   3   0   0 352

Overall Statistics
                                         
               Accuracy : 0.9827         
                 95% CI : (0.9758, 0.988)
    No Information Rate : 0.2847         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9781         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9964   0.9789   0.9795   0.9720   0.9778
Specificity            0.9964   0.9962   0.9913   0.9963   0.9981
Pos Pred Value         0.9911   0.9841   0.9599   0.9811   0.9915
Neg Pred Value         0.9986   0.9949   0.9957   0.9945   0.9950
Prevalence             0.2847   0.1934   0.1745   0.1638   0.1837
Detection Rate         0.2837   0.1893   0.1709   0.1592   0.1796
Detection Prevalence   0.2862   0.1923   0.1781   0.1622   0.1811
Balanced Accuracy      0.9964   0.9875   0.9854   0.9842   0.9880
```

As you can see, the accuracy of this model is quite good for the test set (98%).

This model had 100% accuracy on the Coursera test set (20 rows) submitted as part 2 of this project.